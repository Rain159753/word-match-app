import streamlit as st
import spacy
import re
from collections import Counter
import pandas as pd
import io
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import requests
from streamlit_lottie import st_lottie

# ==========================================
# 0. é­”æ³•å‡½æ•°ï¼šåŠ è½½ Lottie åŠ¨ç”»
# ==========================================
@st.cache_data
def load_lottieurl(url: str):
    try:
        r = requests.get(url)
        if r.status_code != 200:
            return None
        return r.json()
    except:
        return None

# ==========================================
# 1. é¡µé¢é…ç½® & ä¾§è¾¹æ 
# ==========================================
st.set_page_config(
    page_title="æ™ºèƒ½ä¹¦ç±åˆ†æå¼•æ“",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- åˆå§‹åŒ– Session State (ç»™ç¨‹åºå®‰ä¸ªè®°å¿†è„‘) ---
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None # ç”¨æ¥å­˜ç”±äºå•è¯è®¡æ•°ç»“æœ

with st.sidebar:
    st.image("https://cdn-icons-png.flaticon.com/512/2097/2097055.png", width=80) 
    st.title("æ§åˆ¶ä¸­æ¢ âš™ï¸")
    st.markdown("---")
    
    st.subheader("1. æ•°æ®æºæ¥å…¥ ğŸ“–")
    book_file = st.file_uploader("ä¸Šä¼ ç›®æ ‡ä¹¦ç± (TXTæ ¼å¼)", type=["txt"])
    
    st.subheader("2. çŸ¥è¯†åº“å¯¹æ¥ ğŸ“")
    vocab_files = st.file_uploader("ä¸Šä¼ å‚è€ƒè¯è¡¨ (TXTæ ¼å¼ï¼Œå¯å¤šé€‰)", type=["txt"], accept_multiple_files=True)
    
    st.markdown("---")
    
    # è¿™é‡Œçš„æŒ‰é’®åªè´Ÿè´£â€œè§¦å‘è®¡ç®—â€
    run_button = st.button("ğŸš€ å¯åŠ¨åˆ†æå¼•æ“", type="primary", use_container_width=True)

# ==========================================
# 2. ä¸»é¡µé¢å†…å®¹ (æ ‡é¢˜ä¿®æ”¹åŒº)
# ==========================================

lottie_tech = load_lottieurl("https://assets10.lottiefiles.com/packages/lf20_qp1q7mct.json")

col_hero_1, col_hero_2 = st.columns([1, 2])

with col_hero_1:
    if lottie_tech:
        st_lottie(lottie_tech, height=200, key="tech_anim")

with col_hero_2:
    st.markdown("""
        <h1 style='display: inline-block; margin-bottom: 0;'>æ™ºèƒ½æ–‡æœ¬æ•°æ®åˆ†æå¹³å°</h1>
        <span style='font-size: 1rem; color: #808080; margin-left: 10px;'> â€” powered by Zeno</span>
    """, unsafe_allow_html=True)
    
    st.markdown("""
    <div style='background-color: #1E1E1E; padding: 15px; border-radius: 10px; border-left: 5px solid #FF4B4B; margin-top: 20px;'>
        <p style='font-size: 16px; color: #FAFAFA; margin: 0;'>
        æ¬¢è¿ä½¿ç”¨ä¸‹ä¸€ä»£æ–‡æœ¬æ´å¯Ÿå·¥å…·ã€‚å€ŸåŠ©å…ˆè¿›çš„ NLP æŠ€æœ¯ï¼Œæˆ‘ä»¬å°†éç»“æ„åŒ–æ–‡æœ¬è½¬åŒ–ä¸ºå¯è§†åŒ–çš„æ•°æ®èµ„äº§ã€‚
        <br><b>è¯·åœ¨å·¦ä¾§æ§åˆ¶ä¸­æ¢ä¸Šä¼ æ‚¨çš„æ•°æ®ä»¥å¼€å§‹æ¢ç´¢ã€‚</b>
        </p>
    </div>
    """, unsafe_allow_html=True)

st.divider()

# ==========================================
# 3. åŠ è½½æ¨¡å‹
# ==========================================
@st.cache_resource
def load_model():
    try:
        nlp = spacy.load("en_core_web_sm", exclude=["parser", "ner"])
    except TypeError:
        nlp = spacy.load("en_core_web_sm")
        for pipe in ("parser", "ner"):
            if pipe in nlp.pipe_names:
                try:
                    nlp.remove_pipe(pipe)
                except Exception:
                    pass
    return nlp

if 'nlp' not in st.session_state:
    with st.spinner('æ­£åœ¨åˆå§‹åŒ– AI å†…æ ¸...'):
        st.session_state.nlp = load_model()

# ==========================================
# 4. æ ¸å¿ƒé€»è¾‘ï¼šè§¦å‘è®¡ç®—
# ==========================================
# åªæœ‰ç‚¹å‡»æŒ‰é’®æ—¶ï¼Œæ‰è¿›è¡Œâ€œé‡è®¡ç®—â€ï¼Œå¹¶æŠŠç»“æœå­˜å…¥ session_state
if run_button:
    if not book_file:
        st.error("âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ°ä¹¦ç±æ•°æ®æºã€‚è¯·åœ¨ä¾§è¾¹æ ä¸Šä¼ ã€‚")
    elif not vocab_files:
        st.error("âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ°å‚è€ƒè¯è¡¨ã€‚è¯·åœ¨ä¾§è¾¹æ ä¸Šä¼ ã€‚")
    else:
        # --- A. å¤„ç†ä¹¦ç± ---
        st.subheader("ğŸŸ¢ å®æ—¶å¤„ç†è¿›åº¦")
        status_text = st.empty()
        progress_bar = st.progress(0)
        
        status_text.markdown("**Step 1/2: æ­£åœ¨è§£æåŸå§‹æ–‡æœ¬æµ...**")
        
        # è¯»å–æ–‡ä»¶
        text = book_file.getvalue().decode("utf-8")
        words = re.findall(r"[a-zA-Z]+", text)
        
        CHUNK_WORDS = 50000 
        def iter_chunks_wordlist(wordlist, chunk_words=CHUNK_WORDS):
            for i in range(0, len(wordlist), chunk_words):
                yield " ".join(wordlist[i:i+chunk_words])

        lemmas = []
        total_chunks = (len(words) // CHUNK_WORDS) + 1
        current_chunk = 0
        
        nlp = st.session_state.nlp

        status_text.markdown("**Step 2/2: AI å†…æ ¸æ­£åœ¨è¿›è¡Œè¯­è¨€å­¦ç‰¹å¾æå– (Lemmatization)...**")
        for doc in nlp.pipe(iter_chunks_wordlist(words), batch_size=4):
            current_chunk += 1
            progress_bar.progress(min(current_chunk / total_chunks, 1.0))

            for token in doc:
                if token.is_alpha:
                    lemma = token.lemma_.lower()
                    if token.pos_ == "ADV" and lemma.endswith("ly"):
                        base = lemma[:-2]
                        if len(base) > 2:
                            lemma = base
                    lemmas.append(lemma)
        
        word_counts = Counter(lemmas)
        
        # === å…³é”®ç‚¹ï¼šè®¡ç®—å®Œå­˜å…¥è®°å¿†ï¼Œè€Œä¸æ˜¯ç›´æ¥æ˜¾ç¤º ===
        st.session_state.analysis_results = word_counts
        
        progress_bar.empty()
        status_text.empty() # æ¸…ç†æ‰è¿›åº¦æ–‡å­—
        st.success(f"âœ… åˆ†æå®Œæˆï¼å·²å­˜å…¥ç¼“å­˜ã€‚å…±å‘ç° {len(word_counts)} ä¸ªå”¯ä¸€è¯æ±‡ã€‚")

# ==========================================
# 5. æ˜¾ç¤ºé€»è¾‘ï¼šæ¸²æŸ“ç»“æœ
# ==========================================
# åªè¦è®°å¿†é‡Œæœ‰ç»“æœï¼Œå°±ä¸€ç›´æ˜¾ç¤ºï¼ˆä¸ç®¡ä½ æœ‰æ²¡æœ‰æŒ‰æŒ‰é’®ï¼Œä¸ç®¡ä½ æœ‰æ²¡æœ‰åˆ·æ–°ï¼‰
if st.session_state.analysis_results:
    word_counts = st.session_state.analysis_results
    
    st.header("ğŸ“Š æ•°æ®æ´å¯ŸæŠ¥å‘Š")
    
    # é‡æ–°è¯»å– vocab_files (Streamlit çš„ uploader ä¼šç¼“å­˜æ–‡ä»¶å†…å®¹ï¼Œæ‰€ä»¥æ˜¯å®‰å…¨çš„)
    if vocab_files:
        vocab_names = [v.name for v in vocab_files]
        tabs = st.tabs([f"ğŸ“ {name}" for name in vocab_names])

        for i, v_file in enumerate(vocab_files):
            with tabs[i]:
                vocab_name = v_file.name.split('.')[0]
                # æ¯æ¬¡è¯»å–å‰å¦‚æœä¸é‡ç½®æŒ‡é’ˆï¼Œå¤šæ¬¡è¯»å–å¯èƒ½ä¸ºç©ºï¼Œæ‰€ä»¥ç”¨ getvalue() æœ€ç¨³
                v_content = v_file.getvalue().decode("utf-8")
                vocab_words = set(line.strip().lower() for line in v_content.splitlines() if line.strip())
                
                matched_words = {word: count for word, count in word_counts.items() if word in vocab_words}
                df = pd.DataFrame(matched_words.items(), columns=["Word", "Count"]).sort_values(by="Count", ascending=False)
                
                c1, c2 = st.columns([2, 1]) 
                
                with c1:
                    st.subheader(f"â˜ï¸ {vocab_name} - è¯­ä¹‰äº‘å›¾")
                    if not df.empty:
                        wc = WordCloud(
                            width=800, height=500, 
                            background_color='#0E1117',
                            colormap='plasma',
                            font_path=None
                        ).generate_from_frequencies(matched_words)
                        
                        fig, ax = plt.subplots()
                        fig.patch.set_facecolor('#0E1117')
                        ax.imshow(wc, interpolation='bilinear')
                        ax.axis("off")
                        st.pyplot(fig)
                    else:
                        st.warning("âš ï¸ è¯¥è¯è¡¨ä¸­æœªå‘ç°ä»»ä½•åŒ¹é…é¡¹ã€‚")

                with c2:
                    st.subheader("ğŸ“‹ ç»“æ„åŒ–æ•°æ®æ˜ç»†")
                    
                    if df.empty:
                        st.info("æš‚æ— æ•°æ®")
                    else:
                        # æ’å…¥å‹¾é€‰åˆ—
                        df.insert(0, "Select", False)
                        
                        # å…³é”®ç‚¹ï¼šç»™ data_editor ä¸€ä¸ªå”¯ä¸€çš„ keyï¼Œé˜²æ­¢å®ƒåœ¨é‡ç»˜æ—¶ä¸¢å¤±çŠ¶æ€
                        # æˆ‘ä»¬ç”¨ vocab_name ä½œä¸º key çš„ä¸€éƒ¨åˆ†
                        edited_df = st.data_editor(
                            df,
                            column_config={
                                "Select": st.column_config.CheckboxColumn(
                                    "å¯¼å‡º?",
                                    default=False,
                                )
                            },
                            disabled=["Word", "Count"],
                            hide_index=True,
                            use_container_width=True,
                            height=400,
                            key=f"editor_{vocab_name}" 
                        )
                        
                        selected_rows = edited_df[edited_df["Select"] == True]
                        export_data = selected_rows.drop(columns=["Select"])
                        
                        st.caption(f"å·²é€‰æ‹© {len(export_data)} ä¸ªå•è¯å‡†å¤‡å¯¼å‡º")
                        
                        if not export_data.empty:
                            output = io.BytesIO()
                            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                                export_data.to_excel(writer, index=False)
                            processed_data = output.getvalue()
                            
                            st.download_button(
                                f"ğŸ“¥ å¯¼å‡ºå·²é€‰æ•°æ® (.xlsx)",
                                data=processed_data,
                                file_name=f"{vocab_name}_selected.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                use_container_width=True,
                                type="primary"
                            )
                        else:
                            st.download_button(
                                "ğŸ“¥ è¯·å…ˆå‹¾é€‰å•è¯",
                                data=b"",
                                disabled=True,
                                use_container_width=True
                            )

# ==========================================
# 6. æ³¨å…¥é¡µè„š
# ==========================================
footer_css = """
<style>
.footer {
    position: fixed;
    left: 20px;
    bottom: 20px;
    width: auto;
    background-color: transparent;
    color: #808080;
    text-align: left;
    z-index: 999;
    font-family: sans-serif;
    font-size: 14px;
    pointer-events: none;
}
</style>
<div class="footer">
    <p>âš¡ Powered by <b>Zeno</b></p>
</div>
"""
st.markdown(footer_css, unsafe_allow_html=True)
