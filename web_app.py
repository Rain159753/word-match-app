import streamlit as st
import spacy
import re
from collections import Counter
import pandas as pd
import io
from wordcloud import WordCloud
import requests
from streamlit_lottie import st_lottie

# ==========================================
# 0. é­”æ³•å‡½æ•°åŒº
# ==========================================

@st.cache_data
def load_lottieurl(url: str):
    try:
        r = requests.get(url)
        if r.status_code != 200:
            return None
        return r.json()
    except:
        return None

# ä¿®æ”¹ç‚¹ï¼šèƒŒæ™¯æ”¹å›ç™½è‰²ï¼Œé€‚åº”äº®è‰²ä¸»é¢˜
@st.cache_data 
def generate_wordcloud_image(word_counts):
    wc = WordCloud(
        width=800, height=500, 
        background_color='white', # æ”¹å›ç™½è‰²èƒŒæ™¯
        colormap='viridis', # æ¢å›ä¸€ä¸ªé€šç”¨çš„é…è‰²ï¼Œåœ¨ç™½åº•ä¸Šä¹Ÿå¾ˆå¥½çœ‹
        font_path=None
    ).generate_from_frequencies(word_counts)
    return wc.to_array()

# ==========================================
# 1. é¡µé¢é…ç½® & ä¾§è¾¹æ 
# ==========================================
st.set_page_config(
    page_title="æ™ºèƒ½ä¹¦ç±åˆ†æå¼•æ“",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None 

with st.sidebar:
    st.image("https://cdn-icons-png.flaticon.com/512/2097/2097055.png", width=80) 
    st.title("æ§åˆ¶ä¸­æ¢ âš™ï¸")
    st.markdown("---")
    
    st.subheader("1. æ•°æ®æºæ¥å…¥ ğŸ“–")
    book_file = st.file_uploader("ä¸Šä¼ ç›®æ ‡ä¹¦ç± (TXTæ ¼å¼)", type=["txt"])
    
    st.subheader("2. çŸ¥è¯†åº“å¯¹æ¥ ğŸ“")
    vocab_files = st.file_uploader("ä¸Šä¼ å‚è€ƒè¯è¡¨ (TXTæ ¼å¼ï¼Œå¯å¤šé€‰)", type=["txt"], accept_multiple_files=True)
    
    st.markdown("---")
    
    run_button = st.button("ğŸš€ å¯åŠ¨åˆ†æå¼•æ“", type="primary", use_container_width=True)

# ==========================================
# 2. ä¸»é¡µé¢å†…å®¹
# ==========================================

lottie_tech = load_lottieurl("https://assets10.lottiefiles.com/packages/lf20_qp1q7mct.json")

col_hero_1, col_hero_2 = st.columns([1, 2])

with col_hero_1:
    if lottie_tech:
        st_lottie(lottie_tech, height=200, key="tech_anim")

with col_hero_2:
    # ä¿®æ”¹ç‚¹ï¼šé¡µè„šç½²åæ›´æ–°
    st.markdown("""
        <h1 style='display: inline-block; margin-bottom: 0;'>æ™ºèƒ½æ–‡æœ¬æ•°æ®åˆ†æå¹³å°</h1>
        <span style='font-size: 1rem; color: #555; margin-left: 10px;'> â€” powered by Gemini&Zeno</span>
    """, unsafe_allow_html=True)
    
    # ä¿®æ”¹ç‚¹ï¼šèƒŒæ™¯è‰²æ”¹ä¸ºæµ…ç° (#F0F2F6)ï¼Œæ–‡å­—æ”¹ä¸ºæ·±ç° (#31333F)
    st.markdown("""
    <div style='background-color: #F0F2F6; padding: 15px; border-radius: 10px; border-left: 5px solid #FF4B4B; margin-top: 20px;'>
        <p style='font-size: 16px; color: #31333F; margin: 0;'>
        æ¬¢è¿ä½¿ç”¨ä¸‹ä¸€ä»£æ–‡æœ¬æ´å¯Ÿå·¥å…·ã€‚å€ŸåŠ©å…ˆè¿›çš„ NLP æŠ€æœ¯ï¼Œæˆ‘ä»¬å°†éç»“æ„åŒ–æ–‡æœ¬è½¬åŒ–ä¸ºå¯è§†åŒ–çš„æ•°æ®èµ„äº§ã€‚
        <br><b>è¯·åœ¨å·¦ä¾§æ§åˆ¶ä¸­æ¢ä¸Šä¼ æ‚¨çš„æ•°æ®ä»¥å¼€å§‹æ¢ç´¢ã€‚</b>
        </p>
    </div>
    """, unsafe_allow_html=True)

st.divider()

# ==========================================
# 3. åŠ è½½æ¨¡å‹
# ==========================================
@st.cache_resource
def load_model():
    try:
        nlp = spacy.load("en_core_web_sm", exclude=["parser", "ner"])
    except TypeError:
        nlp = spacy.load("en_core_web_sm")
        for pipe in ("parser", "ner"):
            if pipe in nlp.pipe_names:
                try:
                    nlp.remove_pipe(pipe)
                except Exception:
                    pass
    return nlp

if 'nlp' not in st.session_state:
    with st.spinner('æ­£åœ¨åˆå§‹åŒ– AI å†…æ ¸...'):
        st.session_state.nlp = load_model()

# ==========================================
# 4. æ ¸å¿ƒé€»è¾‘
# ==========================================
if run_button:
    if not book_file:
        st.error("âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ°ä¹¦ç±æ•°æ®æºã€‚è¯·åœ¨ä¾§è¾¹æ ä¸Šä¼ ã€‚")
    elif not vocab_files:
        st.error("âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ°å‚è€ƒè¯è¡¨ã€‚è¯·åœ¨ä¾§è¾¹æ ä¸Šä¼ ã€‚")
    else:
        st.subheader("ğŸŸ¢ å®æ—¶å¤„ç†è¿›åº¦")
        status_text = st.empty()
        progress_bar = st.progress(0)
        
        status_text.markdown("**Step 1/2: æ­£åœ¨è§£æåŸå§‹æ–‡æœ¬æµ...**")
        
        text = book_file.getvalue().decode("utf-8")
        words = re.findall(r"[a-zA-Z]+", text)
        
        CHUNK_WORDS = 50000 
        def iter_chunks_wordlist(wordlist, chunk_words=CHUNK_WORDS):
            for i in range(0, len(wordlist), chunk_words):
                yield " ".join(wordlist[i:i+chunk_words])

        lemmas = []
        total_chunks = (len(words) // CHUNK_WORDS) + 1
        current_chunk = 0
        
        nlp = st.session_state.nlp

        status_text.markdown("**Step 2/2: AI å†…æ ¸æ­£åœ¨è¿›è¡Œè¯­è¨€å­¦ç‰¹å¾æå– (Lemmatization)...**")
        for doc in nlp.pipe(iter_chunks_wordlist(words), batch_size=4):
            current_chunk += 1
            progress_bar.progress(min(current_chunk / total_chunks, 1.0))

            for token in doc:
                if token.is_alpha:
                    lemma = token.lemma_.lower()
                    if token.pos_ == "ADV" and lemma.endswith("ly"):
                        base = lemma[:-2]
                        if len(base) > 2:
                            lemma = base
                    lemmas.append(lemma)
        
        word_counts = Counter(lemmas)
        
        st.session_state.analysis_results = word_counts
        
        progress_bar.empty()
        status_text.empty()
        st.success(f"âœ… åˆ†æå®Œæˆï¼å·²å­˜å…¥ç¼“å­˜ã€‚å…±å‘ç° {len(word_counts)} ä¸ªå”¯ä¸€è¯æ±‡ã€‚")

# ==========================================
# 5. æ˜¾ç¤ºé€»è¾‘
# ==========================================
if st.session_state.analysis_results:
    word_counts = st.session_state.analysis_results
    
    st.header("ğŸ“Š æ•°æ®æ´å¯ŸæŠ¥å‘Š")
    
    if vocab_files:
        vocab_names = [v.name for v in vocab_files]
        tabs = st.tabs([f"ğŸ“ {name}" for name in vocab_names])

        for i, v_file in enumerate(vocab_files):
            with tabs[i]:
                vocab_name = v_file.name.split('.')[0]
                v_content = v_file.getvalue().decode("utf-8")
                vocab_words = set(line.strip().lower() for line in v_content.splitlines() if line.strip())
                
                matched_words = {word: count for word, count in word_counts.items() if word in vocab_words}
                df = pd.DataFrame(matched_words.items(), columns=["Word", "Count"]).sort_values(by="Count", ascending=False)
                
                c1, c2 = st.columns([2, 1]) 
                
                with c1:
                    st.subheader(f"â˜ï¸ {vocab_name} - è¯­ä¹‰äº‘å›¾")
                    if not df.empty:
                        img_array = generate_wordcloud_image(matched_words)
                        st.image(img_array, use_container_width=True)
                    else:
                        st.warning("âš ï¸ è¯¥è¯è¡¨ä¸­æœªå‘ç°ä»»ä½•åŒ¹é…é¡¹ã€‚")

                with c2:
                    st.subheader("ğŸ“‹ ç»“æ„åŒ–æ•°æ®æ˜ç»†")
                    
                    if df.empty:
                        st.info("æš‚æ— æ•°æ®")
                    else:
                        df.insert(0, "Select", False)
                        
                        edited_df = st.data_editor(
                            df,
                            column_config={
                                "Select": st.column_config.CheckboxColumn(
                                    "å¯¼å‡º?",
                                    default=False,
                                )
                            },
                            disabled=["Word", "Count"],
                            hide_index=True,
                            use_container_width=True,
                            height=400,
                            key=f"editor_{vocab_name}" 
                        )
                        
                        selected_rows = edited_df[edited_df["Select"] == True]
                        export_data = selected_rows.drop(columns=["Select"])
                        
                        st.caption(f"å·²é€‰æ‹© {len(export_data)} ä¸ªå•è¯å‡†å¤‡å¯¼å‡º")
                        
                        if not export_data.empty:
                            output = io.BytesIO()
                            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                                export_data.to_excel(writer, index=False)
                            processed_data = output.getvalue()
                            
                            st.download_button(
                                f"ğŸ“¥ å¯¼å‡ºå·²é€‰æ•°æ® (.xlsx)",
                                data=processed_data,
                                file_name=f"{vocab_name}_selected.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                use_container_width=True,
                                type="primary",
                                key=f"dl_btn_ok_{vocab_name}" 
                            )
                        else:
                            st.download_button(
                                "ğŸ“¥ è¯·å…ˆå‹¾é€‰å•è¯",
                                data=b"",
                                disabled=True,
                                use_container_width=True,
                                key=f"dl_btn_disabled_{vocab_name}" 
                            )

# ==========================================
# 6. æ³¨å…¥é¡µè„š
# ==========================================
# ä¿®æ”¹ç‚¹ï¼šæ›´æ–°äº†åå­—
footer_css = """
<style>
.footer {
    position: fixed;
    left: 20px;
    bottom: 20px;
    width: auto;
    background-color: transparent;
    color: #888; /* ç¨å¾®æ·±ä¸€ç‚¹çš„ç°è‰² */
    text-align: left;
    z-index: 999;
    font-family: sans-serif;
    font-size: 14px;
    pointer-events: none;
}
</style>
<div class="footer">
    <p>âš¡ Powered by <b>Gemini & Zeno</b></p>
</div>
"""
st.markdown(footer_css, unsafe_allow_html=True)
