import streamlit as st
import spacy
import re
from collections import Counter
import pandas as pd
import io
import matplotlib.pyplot as plt  # æ–°å¢ï¼šç”¨æ¥ç”»å›¾
from wordcloud import WordCloud  # æ–°å¢ï¼šç”¨æ¥ç”Ÿæˆè¯äº‘

# ==========================================
# 1. é¡µé¢é…ç½® & ä¾§è¾¹æ è®¾è®¡
# ==========================================
st.set_page_config(page_title="ä¹¦ç±è¯æ±‡å¤§ä¾¦æ¢", page_icon="ğŸ•µï¸", layout="wide") # layout="wide" è®©é¡µé¢å˜å®½

# --- ä¾§è¾¹æ  (Sidebar) ---
with st.sidebar:
    st.image("https://cdn-icons-png.flaticon.com/512/2097/2097055.png", width=100) # åŠ ä¸ªè£…é¥°å°å›¾æ ‡
    st.title("æ§åˆ¶é¢æ¿ âš™ï¸")
    st.markdown("åœ¨è¿™é‡Œä¸Šä¼ ä½ çš„æ–‡ä»¶")
    
    st.subheader("1. ä¸Šä¼ ä¹¦ç± ğŸ“–")
    book_file = st.file_uploader("é€‰æ‹©ä¹¦ç± (txt)", type=["txt"])
    
    st.subheader("2. ä¸Šä¼ è¯è¡¨ ğŸ“")
    vocab_files = st.file_uploader("é€‰æ‹©è¯è¡¨ (å¯å¤šé€‰)", type=["txt"], accept_multiple_files=True)
    
    st.info("æç¤ºï¼šè¯äº‘å›¾ç”Ÿæˆå¯èƒ½éœ€è¦å‡ ç§’é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚")
    
    # æ”¾ç½®ä¸€ä¸ªå¼€å§‹æŒ‰é’®
    run_button = st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary", use_container_width=True)

# ==========================================
# 2. ä¸»é¡µé¢å†…å®¹
# ==========================================
st.title("ğŸ“š ä¹¦ç±è¯é¢‘ & è¯äº‘å¯è§†åŒ–å·¥å…·")
st.markdown("""
è¿™ä¸ªå·¥å…·å¯ä»¥å¸®ä½ åˆ†æä¸€æœ¬è‹±æ–‡ä¹¦ä¸­ï¼ŒåŒ…å«äº†å¤šå°‘ä¸ªä½ æŒ‡å®šè¯è¡¨é‡Œçš„å•è¯ã€‚
**å·¦ä¾§ä¸Šä¼ æ–‡ä»¶ï¼Œå³ä¾§æŸ¥çœ‹ç‚«é…·çš„åˆ†ææŠ¥å‘Šï¼**
""")

# ==========================================
# 3. åŠ è½½æ¨¡å‹
# ==========================================
@st.cache_resource
def load_model():
    try:
        nlp = spacy.load("en_core_web_sm", exclude=["parser", "ner"])
    except TypeError:
        nlp = spacy.load("en_core_web_sm")
        for pipe in ("parser", "ner"):
            if pipe in nlp.pipe_names:
                try:
                    nlp.remove_pipe(pipe)
                except Exception:
                    pass
    return nlp

# é¢„åŠ è½½æ¨¡å‹ï¼Œé¿å…ç‚¹å‡»æŒ‰é’®æ—¶å¡é¡¿
if 'nlp' not in st.session_state:
    with st.spinner('æ­£åœ¨å”¤é†’ AI å¼•æ“...'):
        st.session_state.nlp = load_model()

# ==========================================
# 4. æ ¸å¿ƒé€»è¾‘
# ==========================================
if run_button:
    if not book_file:
        st.error("âŒ è¿˜æ²¡æœ‰ä¸Šä¼ ä¹¦ç±å“¦ï¼è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ ä¸Šä¼ ã€‚")
    elif not vocab_files:
        st.error("âŒ è¿˜æ²¡æœ‰ä¸Šä¼ è¯è¡¨å“¦ï¼è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ ä¸Šä¼ ã€‚")
    else:
        # --- A. å¤„ç†ä¹¦ç± ---
        st.divider()
        status_text = st.empty() # åˆ›å»ºä¸€ä¸ªç©ºä½ç”¨æ¥æ˜¾ç¤ºçŠ¶æ€
        progress_bar = st.progress(0)
        
        status_text.write("â³ æ­£åœ¨é˜…è¯»ä¹¦ç±å†…å®¹...")
        
        text = book_file.getvalue().decode("utf-8")
        words = re.findall(r"[a-zA-Z]+", text)
        
        CHUNK_WORDS = 50000 
        def iter_chunks_wordlist(wordlist, chunk_words=CHUNK_WORDS):
            for i in range(0, len(wordlist), chunk_words):
                yield " ".join(wordlist[i:i+chunk_words])

        lemmas = []
        total_chunks = (len(words) // CHUNK_WORDS) + 1
        current_chunk = 0
        
        nlp = st.session_state.nlp

        status_text.write("ğŸ§  AI æ­£åœ¨åˆ†æå•è¯åŸå½¢...")
        for doc in nlp.pipe(iter_chunks_wordlist(words), batch_size=4):
            current_chunk += 1
            progress_bar.progress(min(current_chunk / total_chunks, 1.0))

            for token in doc:
                if token.is_alpha:
                    lemma = token.lemma_.lower()
                    if token.pos_ == "ADV" and lemma.endswith("ly"):
                        base = lemma[:-2]
                        if len(base) > 2:
                            lemma = base
                    lemmas.append(lemma)
        
        word_counts = Counter(lemmas)
        status_text.success(f"âœ… ä¹¦ç±å¤„ç†å®Œæˆï¼å…±å‘ç° {len(word_counts)} ä¸ªå”¯ä¸€å•è¯ã€‚")
        
        # --- B. åŒ¹é…ä¸å¯è§†åŒ– ---
        st.header("ğŸ“Š åˆ†ææŠ¥å‘Š")
        
        # ä½¿ç”¨ Tabs (æ ‡ç­¾é¡µ) æ¥åˆ†å¼€å±•ç¤ºä¸åŒçš„è¯è¡¨ç»“æœ
        vocab_names = [v.name for v in vocab_files]
        tabs = st.tabs(vocab_names) # åŠ¨æ€åˆ›å»ºæ ‡ç­¾é¡µ

        for i, v_file in enumerate(vocab_files):
            with tabs[i]: # åœ¨å¯¹åº”çš„æ ‡ç­¾é¡µé‡Œç”»å›¾
                vocab_name = v_file.name.split('.')[0]
                v_content = v_file.getvalue().decode("utf-8")
                vocab_words = set(line.strip().lower() for line in v_content.splitlines() if line.strip())
                
                matched_words = {word: count for word, count in word_counts.items() if word in vocab_words}
                df = pd.DataFrame(matched_words.items(), columns=["Word", "Count"]).sort_values(by="Count", ascending=False)
                
                # å¸ƒå±€ï¼šå·¦è¾¹æ”¾å›¾è¡¨ï¼Œå³è¾¹æ”¾æ•°æ®è¡¨
                c1, c2 = st.columns([2, 1]) 
                
                with c1:
                    st.subheader(f"â˜ï¸ {vocab_name} è¯äº‘å›¾")
                    if not df.empty:
                        # ç”Ÿæˆè¯äº‘
                        wc = WordCloud(
                            width=800, height=500, 
                            background_color='white',
                            colormap='viridis' # é¢œè‰²é£æ ¼
                        ).generate_from_frequencies(matched_words)
                        
                        # æ˜¾ç¤ºè¯äº‘
                        fig, ax = plt.subplots()
                        ax.imshow(wc, interpolation='bilinear')
                        ax.axis("off") # ä¸æ˜¾ç¤ºåæ ‡è½´
                        st.pyplot(fig)
                    else:
                        st.warning("æ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•å•è¯ï¼Œæ— æ³•ç”Ÿæˆè¯äº‘ã€‚")

                with c2:
                    st.subheader("ğŸ“‹ è¯¦ç»†æ•°æ®")
                    st.dataframe(df, use_container_width=True, height=400)
                    
                    # ä¸‹è½½æŒ‰é’®
                    output = io.BytesIO()
                    with pd.ExcelWriter(output, engine='openpyxl') as writer:
                        df.to_excel(writer, index=False)
                    processed_data = output.getvalue()
                    
                    st.download_button(
                        f"ğŸ“¥ ä¸‹è½½ Excel",
                        data=processed_data,
                        file_name=f"{vocab_name}_result.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        use_container_width=True
                    )