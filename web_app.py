import streamlit as st
import spacy
import re
from collections import Counter
import pandas as pd
import io
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import requests  # æ–°å¢ï¼šç”¨æ¥ä¸‹è½½åŠ¨ç”»æ–‡ä»¶
from streamlit_lottie import st_lottie # æ–°å¢ï¼šç”¨æ¥æ’­æ”¾åŠ¨ç”»

# ==========================================
# 0. é­”æ³•å‡½æ•°ï¼šåŠ è½½ Lottie åŠ¨ç”»
# ==========================================
# è¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„å‡½æ•°ï¼Œç»™å®šä¸€ä¸ª URLï¼Œå®ƒä¼šæŠŠåŠ¨ç”»æ•°æ®æŠ“å–ä¸‹æ¥
@st.cache_data # åŠ ä¸ªç¼“å­˜ï¼Œé¿å…æ¯æ¬¡åˆ·æ–°éƒ½é‡æ–°ä¸‹è½½
def load_lottieurl(url: str):
    r = requests.get(url)
    if r.status_code != 200:
        return None
    return r.json()

# ==========================================
# 1. é¡µé¢é…ç½® & ä¾§è¾¹æ è®¾è®¡
# ==========================================
st.set_page_config(
    page_title="æ™ºèƒ½ä¹¦ç±åˆ†æå¼•æ“", # æ”¹ä¸ªæ›´é«˜å¤§ä¸Šçš„åå­—
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- ä¾§è¾¹æ  (Sidebar) ---
with st.sidebar:
    # è¿™é‡Œä¹Ÿå¯ä»¥æ¢æˆä¸€ä¸ªæ›´é…·çš„ç§‘æŠ€æ„Ÿ Logo å›¾ç‰‡é“¾æ¥
    st.image("https://cdn-icons-png.flaticon.com/512/2097/2097055.png", width=80) 
    st.title("æ§åˆ¶ä¸­æ¢ âš™ï¸")
    st.markdown("---") # åˆ†å‰²çº¿
    
    st.subheader("1. æ•°æ®æºæ¥å…¥ ğŸ“–")
    book_file = st.file_uploader("ä¸Šä¼ ç›®æ ‡ä¹¦ç± (TXTæ ¼å¼)", type=["txt"])
    
    st.subheader("2. çŸ¥è¯†åº“å¯¹æ¥ ğŸ“")
    vocab_files = st.file_uploader("ä¸Šä¼ å‚è€ƒè¯è¡¨ (TXTæ ¼å¼ï¼Œå¯å¤šé€‰)", type=["txt"], accept_multiple_files=True)
    
    st.markdown("---")
    
    # æ”¾ç½®ä¸€ä¸ªå¼€å§‹æŒ‰é’®ï¼ŒåŠ ä¸ªä¸åŒé¢œè‰²çš„æç¤º
    run_button = st.button("ğŸš€ å¯åŠ¨åˆ†æå¼•æ“", type="primary", use_container_width=True)
    if run_button:
         st.caption("å¼•æ“æ­£åœ¨é¢„çƒ­ï¼Œå³å°†å¼€å§‹è®¡ç®—...")


# ==========================================
# 2. ä¸»é¡µé¢å†…å®¹ (é¢œå€¼å‡çº§åŒº)
# ==========================================

# --- A. å¤´éƒ¨ Hero åŒºåŸŸ (åŠ¨ç”» + æ ‡é¢˜) ---
# åŠ è½½ä¸€ä¸ªé…·ç‚«çš„ç§‘æŠ€æ„Ÿ Lottie åŠ¨ç”» (è¿™æ˜¯ä¸€ä¸ªå…è´¹çš„ç¤ºä¾‹åœ°å€)
lottie_tech = load_lottieurl("https://assets10.lottiefiles.com/packages/lf20_qp1q7mct.json")

col_hero_1, col_hero_2 = st.columns([1, 2]) # å·¦çª„å³å®½

with col_hero_1:
    # åœ¨å·¦ä¾§æ˜¾ç¤ºåŠ¨ç”»
    if lottie_tech:
        st_lottie(lottie_tech, height=200, key="tech_anim")

with col_hero_2:
    # åœ¨å³ä¾§æ˜¾ç¤ºå¤§æ ‡é¢˜
    st.title("æ™ºèƒ½æ–‡æœ¬æ•°æ®åˆ†æå¹³å°")
    st.markdown("""
    <div style='background-color: #1E1E1E; padding: 15px; border-radius: 10px; border-left: 5px solid #FF4B4B;'>
        <p style='font-size: 16px; color: #FAFAFA;'>
        æ¬¢è¿ä½¿ç”¨ä¸‹ä¸€ä»£æ–‡æœ¬æ´å¯Ÿå·¥å…·ã€‚å€ŸåŠ©å…ˆè¿›çš„ NLP æŠ€æœ¯ï¼Œæˆ‘ä»¬å°†éç»“æ„åŒ–æ–‡æœ¬è½¬åŒ–ä¸ºå¯è§†åŒ–çš„æ•°æ®èµ„äº§ã€‚
        <br><b>è¯·åœ¨å·¦ä¾§æ§åˆ¶ä¸­æ¢ä¸Šä¼ æ‚¨çš„æ•°æ®ä»¥å¼€å§‹æ¢ç´¢ã€‚</b>
        </p>
    </div>
    """, unsafe_allow_html=True) # ä½¿ç”¨äº†ä¸€ç‚¹ HTML/CSS æ¥ç¾åŒ–æ–‡å­—æ¡†

st.divider()

# ==========================================
# 3. åŠ è½½æ¨¡å‹ (ä¿æŒä¸å˜)
# ==========================================
@st.cache_resource
def load_model():
    try:
        nlp = spacy.load("en_core_web_sm", exclude=["parser", "ner"])
    except TypeError:
        nlp = spacy.load("en_core_web_sm")
        for pipe in ("parser", "ner"):
            if pipe in nlp.pipe_names:
                try:
                    nlp.remove_pipe(pipe)
                except Exception:
                    pass
    return nlp

# é¢„åŠ è½½æ¨¡å‹
if 'nlp' not in st.session_state:
    # è¿™é‡Œç”¨ä¸€ä¸ªç©ºçš„å ä½ç¬¦ï¼Œè®©åŠ è½½è¿‡ç¨‹ä¸é‚£ä¹ˆçªå…€
    with st.spinner('æ­£åœ¨åˆå§‹åŒ– AI å†…æ ¸...'):
        st.session_state.nlp = load_model()

# ==========================================
# 4. æ ¸å¿ƒé€»è¾‘ (é€»è¾‘ä¸å˜ï¼Œåªå¾®è°ƒäº†æç¤ºæ–‡æ¡ˆ)
# ==========================================
if run_button:
    if not book_file:
        st.error("âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ°ä¹¦ç±æ•°æ®æºã€‚è¯·åœ¨ä¾§è¾¹æ ä¸Šä¼ ã€‚")
    elif not vocab_files:
        st.error("âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ°å‚è€ƒè¯è¡¨ã€‚è¯·åœ¨ä¾§è¾¹æ ä¸Šä¼ ã€‚")
    else:
        # --- A. å¤„ç†ä¹¦ç± ---
        st.subheader("ğŸŸ¢ å®æ—¶å¤„ç†è¿›åº¦")
        status_text = st.empty()
        progress_bar = st.progress(0)
        
        status_text.markdown("**Step 1/2: æ­£åœ¨è§£æåŸå§‹æ–‡æœ¬æµ...**")
        
        text = book_file.getvalue().decode("utf-8")
        words = re.findall(r"[a-zA-Z]+", text)
        
        CHUNK_WORDS = 50000 
        def iter_chunks_wordlist(wordlist, chunk_words=CHUNK_WORDS):
            for i in range(0, len(wordlist), chunk_words):
                yield " ".join(wordlist[i:i+chunk_words])

        lemmas = []
        total_chunks = (len(words) // CHUNK_WORDS) + 1
        current_chunk = 0
        
        nlp = st.session_state.nlp

        status_text.markdown("**Step 2/2: AI å†…æ ¸æ­£åœ¨è¿›è¡Œè¯­è¨€å­¦ç‰¹å¾æå– (Lemmatization)...**")
        for doc in nlp.pipe(iter_chunks_wordlist(words), batch_size=4):
            current_chunk += 1
            progress_bar.progress(min(current_chunk / total_chunks, 1.0))

            for token in doc:
                if token.is_alpha:
                    lemma = token.lemma_.lower()
                    if token.pos_ == "ADV" and lemma.endswith("ly"):
                        base = lemma[:-2]
                        if len(base) > 2:
                            lemma = base
                    lemmas.append(lemma)
        
        word_counts = Counter(lemmas)
        progress_bar.empty() # å¤„ç†å®Œåéšè—è¿›åº¦æ¡ï¼Œæ›´æ¸…çˆ½
        # ç”¨ä¸€ä¸ªæ¼‚äº®çš„æˆåŠŸæç¤ºæ¡†
        st.success(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæ¯•ï¼æˆåŠŸå»ºç«‹ç´¢å¼•ï¼ŒåŒ…å« {len(word_counts)} ä¸ªå”¯ä¸€è¯æ±‡åŸºå…ƒã€‚")
        
        # --- B. åŒ¹é…ä¸å¯è§†åŒ– ---
        st.header("ğŸ“Š æ•°æ®æ´å¯ŸæŠ¥å‘Š")
        
        vocab_names = [v.name for v in vocab_files]
        tabs = st.tabs([f"ğŸ“ {name}" for name in vocab_names]) # ç»™æ ‡ç­¾é¡µåŠ ä¸ªå°å›¾æ ‡

        for i, v_file in enumerate(vocab_files):
            with tabs[i]:
                vocab_name = v_file.name.split('.')[0]
                v_content = v_file.getvalue().decode("utf-8")
                vocab_words = set(line.strip().lower() for line in v_content.splitlines() if line.strip())
                
                matched_words = {word: count for word, count in word_counts.items() if word in vocab_words}
                df = pd.DataFrame(matched_words.items(), columns=["Word", "Count"]).sort_values(by="Count", ascending=False)
                
                c1, c2 = st.columns([2, 1]) 
                
                with c1:
                    st.subheader(f"â˜ï¸ {vocab_name} - è¯­ä¹‰äº‘å›¾")
                    if not df.empty:
                        # è°ƒæ•´äº†è¯äº‘èƒŒæ™¯è‰²ï¼Œé€‚åº”æš—é»‘æ¨¡å¼
                        wc = WordCloud(
                            width=800, height=500, 
                            background_color='#0E1117', # é…åˆæš—é»‘èƒŒæ™¯
                            colormap='plasma', # æ¢ä¸ªæ›´ç§‘æŠ€æ„Ÿçš„é…è‰²
                            font_path=None # å¦‚æœæœ‰ä¸­æ–‡å­—ä½“éœ€æ±‚éœ€æŒ‡å®š
                        ).generate_from_frequencies(matched_words)
                        
                        fig, ax = plt.subplots()
                        fig.patch.set_facecolor('#0E1117') # è®¾ç½®å›¾ç‰‡èƒŒæ™¯é€æ˜/é»‘è‰²
                        ax.imshow(wc, interpolation='bilinear')
                        ax.axis("off")
                        st.pyplot(fig)
                    else:
                        st.warning("âš ï¸ è¯¥è¯è¡¨ä¸­æœªå‘ç°ä»»ä½•åŒ¹é…é¡¹ã€‚")

                with c2:
                    st.subheader("ğŸ“‹ ç»“æ„åŒ–æ•°æ®æ˜ç»†")
                    st.dataframe(df, use_container_width=True, height=400)
                    
                    output = io.BytesIO()
                    with pd.ExcelWriter(output, engine='openpyxl') as writer:
                        df.to_excel(writer, index=False)
                    processed_data = output.getvalue()
                    
                    st.download_button(
                        f"ğŸ“¥ å¯¼å‡º {vocab_name} æ•°æ®é›† (.xlsx)",
                        data=processed_data,
                        file_name=f"{vocab_name}_analysis_report.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        use_container_width=True,
                        type="secondary" # æŒ‰é’®æ ·å¼è®¾ä¸ºæ¬¡è¦ï¼Œä¸æŠ¢ä¸»æŒ‰é’®é£å¤´
                    )

# ==========================================
# 5. æ³¨å…¥é¡µè„š (é­”æ³• CSS)
# ==========================================
# è¿™æ®µæ˜¯çº¯ HTML/CSS ä»£ç ï¼Œç”¨æ¥æŠŠæ–‡å­—å›ºå®šåœ¨å·¦ä¸‹è§’
footer_css = """
<style>
.footer {
    position: fixed;
    left: 20px;
    bottom: 20px;
    width: auto;
    background-color: transparent;
    color: #808080; /* ç°è‰²å­—ä½“ï¼Œä½è°ƒä¸€ç‚¹ */
    text-align: left;
    z-index: 999; /* ä¿è¯æµ®åœ¨æœ€ä¸Šå±‚ */
    font-family: sans-serif;
    font-size: 14px;
    pointer-events: none; /* é˜²æ­¢æŒ¡ä½åé¢çš„æ“ä½œ */
}
</style>
<div class="footer">
    <p>âš¡ Powered by <b>Gemini</b></p>
</div>
"""
# ä½¿ç”¨ unsafe_allow_html=True å¼ºåˆ¶æ¸²æŸ“è¿™æ®µä»£ç 
st.markdown(footer_css, unsafe_allow_html=True)
